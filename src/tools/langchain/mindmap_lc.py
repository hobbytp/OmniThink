import json
import uuid # For generating unique node IDs
import re # For parsing LLM output
from typing import List, Dict, Optional, Any

# Assuming LangchainBaseChatModel and LangchainBaseRetriever are defined or imported elsewhere
# For now, using Any as placeholder from previous step if not refining imports here.
# from langchain_core.messages import HumanMessage, SystemMessage # Example if needed directly

# Using 'Any' for LM and Retriever types for now for simplicity in this initial step.
# These can be refined with BaseChatModel, BaseRetriever from langchain_core later.
LangchainBaseChatModel = Any
LangchainBaseRetriever = Any

class Node:
    """
    Represents a node in the mind map.
    """
    def __init__(self, id: Optional[str] = None, name: str = "", level: int = 0, parent: Optional['Node'] = None):
        """
        Initializes a Node.

        Args:
            id (Optional[str], optional): The unique ID of the node.
                                          If None, a new UUID will be generated. Defaults to None.
            name (str, optional): The name or concept of the node. Defaults to "".
            level (int, optional): The depth level of the node in the mind map. Defaults to 0.
            parent (Optional['Node'], optional): The parent node. Defaults to None.
        """
        self.id: str = id if id else str(uuid.uuid4())
        self.name: str = name
        self.level: int = level
        self.parent: Optional['Node'] = parent
        self.children: List['Node'] = []
        self.retrieved_snippets: List[Dict[str, str]] = [] # List of {'title': ..., 'text': ...}
        self.summary: Optional[str] = None # Summary of the node based on snippets
        self.sub_concepts: List[str] = [] # Raw sub-concepts generated by LLM before becoming child nodes

    def add_child(self, child_node: 'Node'):
        """Adds a child node to this node."""
        self.children.append(child_node)

    def to_dict(self) -> Dict[str, Any]:
        """
        Converts the node and its children to a dictionary format,
        suitable for JSON serialization.
        """
        return {
            "id": self.id,
            "name": self.name,
            "level": self.level,
            "summary": self.summary,
            # "retrieved_snippets": self.retrieved_snippets, # Usually not needed in final mindmap JSON
            # "sub_concepts": self.sub_concepts, # Usually not needed in final mindmap JSON
            "children": [child.to_dict() for child in self.children]
        }

    def __repr__(self) -> str:
        return f"Node(id={self.id}, name='{self.name}', level={self.level}, children_count={len(self.children)})"

class LangchainMindMapGenerator:
    """
    Generates a mind map using LangChain components (LLMs and Retrievers).
    The structure and logic will be analogous to the DSPy-based MindMap generator.
    """
    def __init__(self,
                 gen_concept_lm: LangchainBaseChatModel,
                 retriever: LangchainBaseRetriever,
                 gen_sub_concept_lm: Optional[LangchainBaseChatModel] = None,
                 summarize_lm: Optional[LangchainBaseChatModel] = None,
                 max_depth: int = 2,
                 concepts_per_level: int = 3):
        """
        Initializes the LangchainMindMapGenerator.

        Args:
            gen_concept_lm (LangchainBaseChatModel): LangChain LLM for generating initial concepts/sub-concepts.
            retriever (LangchainBaseRetriever): LangChain Retriever for fetching relevant documents.
            gen_sub_concept_lm (Optional[LangchainBaseChatModel], optional): Specific LLM for sub-concept generation.
                                                                           Defaults to gen_concept_lm.
            summarize_lm (Optional[LangchainBaseChatModel], optional): Specific LLM for summarization.
                                                                     Defaults to gen_concept_lm.
            max_depth (int, optional): Maximum depth of the mind map. Defaults to 2.
            concepts_per_level (int, optional): Number of sub-concepts to generate per node. Defaults to 3.
        """
        self.gen_concept_lm = gen_concept_lm
        self.retriever = retriever
        self.gen_sub_concept_lm = gen_sub_concept_lm or gen_concept_lm
        self.summarize_lm = summarize_lm or gen_concept_lm
        self.max_depth = max_depth
        self.concepts_per_level = concepts_per_level
        self.root: Optional[Node] = None
        # self.node_count = 0 # Using uuid for IDs, so direct count not strictly needed for unique IDs

    def _get_unique_id(self) -> str:
        """Generates a unique ID for a node."""
        return str(uuid.uuid4())

    def _parse_llm_output_to_list(self, text_output: str) -> List[str]:
        """
        Parses a text output expected to be a list (e.g., numbered, bulleted)
        into a Python list of strings.
        """
        if not text_output:
            return []
        lines = text_output.strip().split('\n')
        concepts = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # Try to strip common list item prefixes
            match = re.match(r"^\s*(?:\d+\.\s*|-\s*|\*\s*|\d+\)\s*)\s*(.+)", line)
            if match:
                concept_text = match.group(1).strip()
                # Further clean if the concept itself starts with a list-like marker
                # This can happen if LLM is overly verbose, e.g. "- 1. Concept"
                inner_match = re.match(r"^\s*(?:\d+\.\s*|-\s*|\*\s*|\d+\)\s*)\s*(.+)", concept_text)
                if inner_match:
                    concepts.append(inner_match.group(1).strip())
                else:
                    concepts.append(concept_text)
            elif line: # If no common list pattern matched, but line has content
                concepts.append(line)
        return [c for c in concepts if c] # Filter out any empty strings that might have slipped through

    def _generate_concepts_for_node(self, parent_node_name: str, parent_level: int, context_snippets: Optional[List[Dict[str, str]]] = None) -> List[str]:
        """
        Generates sub-concepts for a given parent node name using an LLM.

        Args:
            parent_node_name (str): The name of the parent node.
            parent_level (int): The level of the parent node.
            context_snippets (Optional[List[Dict[str, str]]]): Optional context snippets
                                                               for generating more specific sub-concepts.

        Returns:
            List[str]: A list of generated concept names.
        """
        prompt_parts = []
        if parent_level == 0: # Root topic
            prompt_parts.append(f"Generate a list of {self.concepts_per_level} main concepts or categories related to the topic: \"{parent_node_name}\".")
        else: # Sub-concept generation
            prompt_parts.append(f"Given the parent concept: \"{parent_node_name}\",")
            if context_snippets:
                formatted_snippets = "\n".join([f"- {s['title']}: {s['text']}" for s in context_snippets[:3]]) # Use top 3 snippets
                prompt_parts.append(f"Consider the following context:\n{formatted_snippets}\n")
            prompt_parts.append(f"Generate a list of {self.concepts_per_level} specific sub-concepts related to \"{parent_node_name}\".")

        prompt_parts.append("Return the concepts as a numbered list, each on a new line (e.g., '1. Concept A').")
        prompt_text = "\n".join(prompt_parts)

        print(f"\n[LLM Prompt for '{parent_node_name}' (L{parent_level})]:\n{prompt_text}\n")

        # Assuming self.gen_sub_concept_lm (or self.gen_concept_lm) is a LangChain BaseChatModel
        # and handles string input directly or requires a list of Messages.
        # For simplicity with current BaseChatModel, sending as a single human message.
        # from langchain_core.messages import HumanMessage # Would be at top-level if used
        # message = HumanMessage(content=prompt_text)
        # response = self.gen_sub_concept_lm.invoke([message])
        # llm_output = response.content if hasattr(response, 'content') else str(response)

        # Using .invoke with string directly, many LC models support this.
        try:
            response_content = self.gen_sub_concept_lm.invoke(prompt_text)
            if hasattr(response_content, 'content'): # AIMessage like
                llm_output = response_content.content
            else: # string like
                llm_output = str(response_content)
        except Exception as e:
            print(f"Error invoking LLM for concept generation: {e}")
            return []


        print(f"[LLM Raw Output for '{parent_node_name}']: {llm_output}")
        generated_concepts = self._parse_llm_output_to_list(llm_output)
        print(f"[LLM Parsed Concepts for '{parent_node_name}']: {generated_concepts}")
        return generated_concepts[:self.concepts_per_level] # Ensure we don't exceed requested number

    def _summarize_node_content(self, node_name: str, snippets: List[Dict[str, str]]) -> Optional[str]:
        """
        Generates a concise summary for a node based on its name and retrieved snippets.

        Args:
            node_name (str): The name of the node being summarized.
            snippets (List[Dict[str, str]]): A list of dictionaries, where each
                                             dictionary has "title" and "text" (snippet).

        Returns:
            Optional[str]: A summary string if successful, otherwise None.
        """
        if not snippets:
            print(f"No snippets provided for '{node_name}', skipping summarization.")
            return None

        # Prepare the context from snippets
        context_parts = []
        for i, snippet in enumerate(snippets[:5]): # Use up to 5 snippets for summary context
            title = snippet.get('title', '')
            text = snippet.get('text', '')
            context_parts.append(f"Snippet {i+1}:\nTitle: {title}\nContent: {text}\n---")

        formatted_snippets = "\n".join(context_parts)

        prompt_text = (
            f"Concept: \"{node_name}\"\n\n"
            f"Retrieved Information:\n{formatted_snippets}\n\n"
            f"Based *only* on the retrieved information provided above, "
            f"write a concise summary (1-2 sentences) for the concept \"{node_name}\". "
            f"If the information is insufficient or irrelevant, state that a summary cannot be formed."
        )

        print(f"\n[LLM Prompt for Summarizing '{node_name}']:\n{prompt_text}\n")

        try:
            # Assuming self.summarize_lm is a LangChain BaseChatModel
            response_content = self.summarize_lm.invoke(prompt_text)
            if hasattr(response_content, 'content'): # AIMessage like
                summary_text = response_content.content.strip()
            else: # string like
                summary_text = str(response_content).strip()

            print(f"[LLM Raw Summary for '{node_name}']: {summary_text}")

            # Basic check if summary is meaningful (e.g., not just a refusal)
            if not summary_text or "cannot be formed" in summary_text.lower() or "insufficient information" in summary_text.lower():
                print(f"LLM indicated summary cannot be formed for '{node_name}'.")
                return None

            return summary_text
        except Exception as e:
            print(f"Error invoking LLM for summarization of '{node_name}': {e}")
            return None

    def build_map(self, topic: str) -> Optional[Node]:
        """
        Builds the mind map for the given topic using a breadth-first expansion.
        """
        self.root = Node(id=self._get_unique_id(), name=topic, level=0)
        if not self.root: # Should not happen with current Node init
            return None

        queue: List[Node] = [self.root]

        processed_nodes = 0 # To avoid getting stuck in very large maps or loops if any

        while queue:
            current_node = queue.pop(0)
            processed_nodes +=1

            print(f"\nProcessing Node: '{current_node.name}' (Level {current_node.level})")

            if current_node.level >= self.max_depth:
                print(f"Max depth ({self.max_depth}) reached for node '{current_node.name}'. No further expansion.")
                continue

            # Retrieve context snippets for sub-concept generation (not for the main topic's initial concepts)
            # Also, skip retrieval if retriever is not set (e.g. for testing without it)
            if current_node.level >= 0 and self.retriever : # Changed to >=0 to retrieve for root as well, if desired for initial concepts.
                                                        # Or keep >0 if root concepts should be purely generative.
                                                        # For this iteration, let's try retrieving for root too.
                try:
                    print(f"Retrieving snippets for '{current_node.name}'...")
                    current_node.retrieved_snippets = self.retriever.search(query=current_node.name, k=3) # Default k=3 snippets
                    print(f"Retrieved {len(current_node.retrieved_snippets)} snippets.")
                except Exception as e:
                    print(f"Error during snippet retrieval for '{current_node.name}': {e}")
                    current_node.retrieved_snippets = []


            # Generate sub-concepts
            # For level 0 (root), gen_concept_lm is used. For others, gen_sub_concept_lm.
            # This active_llm selection was for _generate_concepts_for_node. Summarize_lm is distinct.
            # active_llm = self.gen_concept_lm if current_node.level == 0 else self.gen_sub_concept_lm

            # Summarize the current node's content based on retrieved snippets
            if current_node.retrieved_snippets:
                print(f"Attempting to summarize content for '{current_node.name}'...")
                summary = self._summarize_node_content(current_node.name, current_node.retrieved_snippets)
                if summary:
                    current_node.summary = summary
                    print(f"Summary for '{current_node.name}': {current_node.summary}")
                else:
                    print(f"No summary generated for '{current_node.name}'.")

            new_concept_names = self._generate_concepts_for_node(
                parent_node_name=current_node.name,
                parent_level=current_node.level,
                context_snippets=current_node.retrieved_snippets
            )
            current_node.sub_concepts = new_concept_names # Store raw generated names

            for concept_name in new_concept_names:
                if not concept_name.strip(): continue # Skip empty names

                child_node = Node(
                    id=self._get_unique_id(),
                    name=concept_name.strip(),
                    level=current_node.level + 1,
                    parent=current_node
                )
                current_node.add_child(child_node)
                print(f"  Added Child: '{child_node.name}' (Level {child_node.level}) to '{current_node.name}'")

                if child_node.level < self.max_depth: # Only add to queue if it needs further expansion
                    queue.append(child_node)

            # Safety break for very large maps during development, can be removed/adjusted
            if processed_nodes > 50 : # Arbitrary limit to prevent accidental infinite loops in dev
                 print("WARN: Processed over 50 nodes, stopping early for safety.")
                 break


        print("\nMind map building process complete.")
        return self.root

    def save_map_to_json(self, filepath: str):
        """
        Saves the generated mind map (from the root node) to a JSON file.
        """
        if not self.root:
            print("Error: Mind map has not been generated yet (root node is None). Cannot save.")
            return

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.root.to_dict(), f, indent=4, ensure_ascii=False)
            print(f"Mind map saved to {filepath}")
        except Exception as e:
            print(f"Error saving mind map to JSON: {e}")

    # Future methods will include:
    # - _generate_sub_concepts(parent_node_name: str, parent_level: int) -> List[str]
    # - _retrieve_and_summarize_node(node: Node)
    # - _build_level_recursively(parent_node: Node, current_depth: int)

# Example usage (for testing the skeleton)
if __name__ == '__main__':
    # This part uses placeholder LMs and Retriever for instantiation testing.
    class PlaceholderChatModel: # Mocking BaseChatModel
        def invoke(self, *args, **kwargs):
            print(f"PlaceholderChatModel.invoke called with: {args}, {kwargs}")
            return "Mocked LLM response for: " + str(args[0]) if args else "Mocked response"

    class PlaceholderRetriever: # Mocking BaseRetriever or a compatible search interface
        def search(self, query: str, k: Optional[int] = None) -> List[Dict[str, str]]:
            print(f"PlaceholderRetriever.search called for query: '{query}', k={k}")
            return [{"title": f"Mock title for {query}", "text": f"Mock snippet for {query}"}]

    print("Testing LangchainMindMapGenerator instantiation and basic methods...")
    try:
        lm_instance = PlaceholderChatModel()
        retriever_instance = PlaceholderRetriever()

        mind_map_generator = LangchainMindMapGenerator(
            gen_concept_lm=lm_instance,
            retriever=retriever_instance,
            max_depth=1, # Keep simple for skeleton test
            concepts_per_level=2
        )
        print("LangchainMindMapGenerator instantiated successfully.")

        test_topic = "Introduction to Quantum Computing"
        print(f"\nAttempting to build a skeleton map for: '{test_topic}'")
        root_node = mind_map_generator.build_map(test_topic)

        if root_node:
            print(f"Root node created: {root_node}")
            # Add a couple of dummy children for testing save functionality
            child1_name = "Qubits"
            child1_node = Node(id=mind_map_generator._get_unique_id(), name=child1_name, level=1, parent=root_node)
            child1_node.summary = "A qubit is the basic unit of quantum information."
            root_node.add_child(child1_node)

            child2_name = "Entanglement"
            child2_node = Node(id=mind_map_generator._get_unique_id(), name=child2_name, level=1, parent=root_node)
            child2_node.summary = "Quantum entanglement is a physical phenomenon..."
            root_node.add_child(child2_node)

            print(f"Manually added child nodes: {root_node.children}")

            output_filename = "test_mindmap_lc_skeleton.json"
            print(f"\nAttempting to save skeleton map to '{output_filename}'")
            mind_map_generator.save_map_to_json(output_filename)

            # Verify file content (optional manual check)
            # try:
            #     with open(output_filename, 'r', encoding='utf-8') as f_check:
            #         saved_data = json.load(f_check)
            #     print(f"Successfully read back saved map: {saved_data['name']} with {len(saved_data['children'])} children.")
            # except Exception as e_read:
            #     print(f"Error reading back saved map: {e_read}")

        else:
            print("Failed to build/get root node for the map.")

    except Exception as e:
        print(f"Error during LangchainMindMapGenerator test: {e}")
        import traceback
        traceback.print_exc()
